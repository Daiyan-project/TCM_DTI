{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Jax models, missing a dependency. jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deepchem as dc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU测试\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "\n",
    "\n",
    "# cmd nvidia-smi -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 定义nested_cv函数\n",
    "# 将数据集划分成三份，进行迭代。首先两份输入inner_cv进行交叉验证，记录基于AUROC的最佳模型，最后一份用于在模型上验证\n",
    "def nested_cv(dataset_X, dataset_y, model, epoch): \n",
    "    # 使用 StratifiedKFold 创建分层交叉验证划分\n",
    "    outer_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    auroc_scores = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(outer_skf.split(dataset_X, dataset_y)):\n",
    "        # 创建训练集和测试集的 NumpyDataset\n",
    "        inner_cv_dataset = dc.data.NumpyDataset(X=dataset_X[train_index], y=dataset_y[train_index])\n",
    "        test_dataset = dc.data.NumpyDataset(X=dataset_X[test_index], y=dataset_y[test_index])\n",
    "        model = inner_cv(inner_cv_dataset, model, epoch)\n",
    "\n",
    "        # 在测试集上进行预测\n",
    "        y_true = test_dataset.y\n",
    "        y_pred = model.predict(test_dataset)[:,1]\n",
    "\n",
    "        # 计算 AUROC 和 AUPR\n",
    "        auroc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        # 保存结果\n",
    "        auroc_scores.append(auroc)\n",
    "\n",
    "    #获取三次的均值\n",
    "    average_rocauc_score = np.mean(auroc_scores)\n",
    "    \n",
    "    return average_rocauc_score\n",
    "\n",
    "#2.1 定义内层循环\n",
    "#真正进行训练的函数\n",
    "def inner_cv(inner_cv_dataset, model, epoch):\n",
    "    inner_skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    auroc_scores = []\n",
    "\n",
    "    X,y = inner_cv_dataset.X, inner_cv_dataset.y\n",
    "\n",
    "    best_models = [] \n",
    "    for fold, (train_index, val_index) in enumerate(inner_skf.split(X, y)):\n",
    "        # 创建训练集和测试集的 NumpyDataset\n",
    "        train_dataset = dc.data.NumpyDataset(X=X[train_index], y=y[train_index])\n",
    "        val_dataset = dc.data.NumpyDataset(X=X[val_index], y=y[val_index])\n",
    "\n",
    "\n",
    "        #模型训练\n",
    "        loss = model.fit(train_dataset, nb_epoch=epoch)\n",
    "\n",
    "        # 在测试集上进行预测\n",
    "        y_true = val_dataset.y\n",
    "        y_pred = model.predict(val_dataset)[:,1]\n",
    "\n",
    "        # 计算AUC-ROC值\n",
    "        aucroc = roc_auc_score(y_true, y_pred)\n",
    "        auroc_scores.append(aucroc)\n",
    "\n",
    "        # 保存全部模型\n",
    "        best_models.append(copy.deepcopy(model))  # 使用深层复制保存当前模型\n",
    "\n",
    "        # 模型初始化：对模型的每个层应用初始化\n",
    "        for module in model.model.model.modules():\n",
    "            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):\n",
    "                weights_init(module)\n",
    "\n",
    "    # 在best_models列表中选择在验证集上性能最好的模型\n",
    "    best_model_index = np.argmax(auroc_scores)\n",
    "    best_model = best_models[best_model_index]\n",
    "    return best_model\n",
    "\n",
    "# 2.1.1 初始化权重和偏置\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 数据输入和特征化\n",
    "#文件路径准备\n",
    "basePath = os.getcwd()\n",
    "resultPath = basePath+'/results'\n",
    "training_data_path = basePath+'/training_data'\n",
    "training_list = os.listdir(training_data_path)\n",
    "\n",
    "algorithm = 'AttentiveFP'\n",
    "\n",
    "#hyperparameters setting\n",
    "graph_conv_layers_values = [[32, 32], [32, 64], [64, 64]]\n",
    "attention_hidden_size_values = [64, 128]\n",
    "dense_layer_size_value = 128\n",
    "dropout_value = 0.5\n",
    "\n",
    "#设置epoch, batch_size参数\n",
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "#设置超参数组合\n",
    "all_combinations = list(itertools.product(graph_conv_layers_values, attention_hidden_size_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traindataset in training_list:\n",
    "\n",
    "    tar_ids = []\n",
    "    final_params = []\n",
    "    final_scores = []\n",
    "\n",
    "    pertarget_files = training_data_path+'/'+traindataset\n",
    "    files_list = os.listdir(pertarget_files)\n",
    "    # print(traindataset)\n",
    "    for tar_id in tqdm(files_list):\n",
    "        smiles = pd.read_csv(training_data_path + '/' +traindataset+'/'+tar_id, header=0,index_col=False)['c_smiles'].tolist()\n",
    "        labels = pd.read_csv(training_data_path + '/' +traindataset+'/'+tar_id, header=0,index_col=False)['active_label'].tolist()\n",
    "        labels = np.array(labels).reshape((len(labels), 1))\n",
    "\n",
    "        # AttentiveFPModel需要使用MolGraphConvFeaturizer进行特征提取\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        X = featurizer.featurize(smiles)\n",
    "        dataset = dc.data.NumpyDataset(X=X, y=labels)\n",
    "\n",
    "        dataset_X = dataset.X\n",
    "        dataset_y = dataset.y\n",
    "\n",
    "        \n",
    "        rocauc_params = []\n",
    "        rocauc_score = []\n",
    "        \n",
    "        for graph_conv_layers, attention_hidden_size in all_combinations:  \n",
    "            # 创建AttentiveFPModel\n",
    "            model = dc.models.AttentiveFPModel(\n",
    "                batch_size=batch_size, \n",
    "                learning_rate=0.001,\n",
    "                n_tasks=1,\n",
    "                mode='classification',\n",
    "                graph_conv_layers=graph_conv_layers,\n",
    "                attention_hidden_size=attention_hidden_size,\n",
    "                dense_layer_size=dense_layer_size_value,\n",
    "                dropout=dropout_value,\n",
    "            )\n",
    "            #AttentiveFPModel的默认优化器（optimizer）是Adam（Adaptive Moment Estimation），l;earning_rate传入adam\n",
    "            #而默认的损失函数（loss function）取决于任务的类型。对于分类任务 (mode='classification')，\n",
    "            #默认使用的是交叉熵损失函数（cross entropy loss）。这些是DeepChem中AttentiveFPModel的默认设置\n",
    "            \n",
    "            model.model.to(device)\n",
    "            #molAttentiveFPModel\n",
    "            \n",
    "            average_rocauc_score = nested_cv(dataset_X, dataset_y, model, epoch)\n",
    "\n",
    "            #记录此时的超参数\n",
    "            params = {\n",
    "            'graph_conv_layers': graph_conv_layers, \n",
    "            'attention_hidden_size': attention_hidden_size,\n",
    "            'dense_layer_size_value': dense_layer_size_value,\n",
    "            'dropout_value': dropout_value\n",
    "            }   \n",
    "            \n",
    "            #记录超参数，记录分数\n",
    "            rocauc_params.append(params)\n",
    "            rocauc_score.append(average_rocauc_score)\n",
    "\n",
    "        #获取最佳分数以及最佳参数\n",
    "        tar_ids.append(tar_id)\n",
    "        final_params.append(rocauc_params[np.argmax(rocauc_score)])\n",
    "        final_scores.append(max(rocauc_score))\n",
    "\n",
    "        #数据输出\n",
    "    data={'targets':tar_ids, 'best_params':final_params, 'rocauc_score':final_scores}\n",
    "    roc_data = pd.DataFrame(data) \n",
    "    roc_data.to_csv(resultPath+'/'+algorithm+'_'+traindataset+'_rocmean.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
